# parallel-sort
Home of the final project for my CSC 139 course. This is a mult-threaded fixed-length record sorting application written in C. 
PSORT is a multi-threaded stable sorting application for fixed-sized records. Records are 128 bytes wide and composed of a 32-bit unsigned integer concatenated with a 124-byte wide data payload. PSORT utilizes merge-sort to achieve efficient sorting and stability. A dynamic work queue is used to keep track of work and assign tasks to threads. Merging of the array is accomplished by a serial merge, trading performance for simplicity in synchronization. This report will discuss the architecture of PSORT, Synchronization design, performance, as well as techniques, mechanisms, and possible improvements. 
# Architecture Summary
PSORT adopts a simple strategy for splitting up work for threads. The input is split into several chunks equal to the number of available CPU cores on the system running PSORT. Each thread is given a number of records equal to (record array length)/(number of chunks) . This has several consequences in terms of performance and portability. First, because the number of threads is capped at the number of available cores on a system, if each thread is given its own core, we will never pay the cost of context switching between PSORT threads. Second, because PSORT uses operating system dependent system calls to determine the number of available cores on a system it will fail to compile on Windows or Apple systems. I considered relating the number of threads to the size of the work being done, but an intelligent way of doing this would be to determine some chunk size K, and dividing the number of records by K. K would need to be determined empirically as we are asking the question “when does threading become worth it”? This would entail conducting tests with larger and larger K values and analyzing the performance of threads and the impact of smaller and larger K values. This is something I plan on doing while on break. 
PSORT utilizes a simple thread model. POSIX threads and the standard library of functions that come along with POSIX threads are used to initialize, create, and manage threads. Tasks are created, enqueued onto a dynamic work queue, and checked out by threads which perform a sort on their designated chunk. The dynamic work queue utilizes separate locks on the head and tail of the queue to improve the performance of enqueue and dequeue operations on the queue. A condition variable is used to signal when work is available for the worker threads to operate on. When work is not available, worker threads wait on the condition variable. The condition variable is signaled on when the parent thread enqueues new work. 
Data flow is straightforward. The user specifies an input file, it is repeatedly read into a 40 Record wide buffer and transferred into an array. The array is continuously resized when necessary. After reading is completed, the array is resized to cut off trailing zeroed memory. Tasks are created and worker threads perform their sort on the array. On their completion parent thread wakes up and performs a serial merge on the array to merge the sorted chunks. After the merge, the array is written to disk in one large write call. 
# Synchronization Design

Threads act on two main shared structures, the work queue and the data array. The work queue is protected explicitly with locks and a condition variable. The array is protected implicitly through the creation of boundaries on which indexes of the array a particular thread is allowed to read and write. The queue holds Tasks which define a start index and an end index. When tasks are added to the queue, the condition variable is signaled on to notify worker threads of available work. Threads wait on the condition variable until work is available. When a thread checks out work from the queue, these indexes are passed to the merge sort function, which only operates on the array within the indexes passed to it. Threads are not spun up until after all tasks are created. I did this because it makes the most sense in my head, but if I had more time I would move thread creation before task creation and see how that affects performance. I suspect it would decrease time to sort, as after a task is created, a thread is immediately ready to begin work, instead of waiting for all tasks to be created. 
# Performance Results
I tested my program on a large range of workload sizes. I used WSL with an AMD Ryzen 5 5600 6 core processor. The clock speed is 4.4GHz. Testing was done by creating many input files of size 1000 records up to 500,000 records and repeatedly executing psort on them. In bytes, this is a range of <1KB to 51MB. I created a 1GB test file, but it seemed like my operating system was terminating the program for using too much memory.  Timing information was calculated by retrieving “wall clock” time using calls to gettimeofday(). There are system calls to get CPU time but online source like Stack Overflow and ChatGPT said these are potentially “not granular enough”. It seemed like CPU time was being “double counted” across threads.  Plots were generated with python using the mathplotlib library. 
 ![Performance on workloads <= 100,000 records](psortpictures/graph1)
This graph illustrates the performance of psort on small workloads up to 100,000 records. Even at this range, the bulk of computational time is taken by the merge and write phase.
 
This graph illustrates the performance of psort on large workloads up to 500,000 records. There are some key insights here. There are very prominent outliers past 375,000 records. I believe this is due to the process being interrupted for longer periods of time while the large I/O takes place. This could be determined by separating the merge and write phase timing into two stages and seeing how I/O behaves as requests get larger. 
![Performance on workloads <= 500,00](psortpictures/graph2)
# Lessons Learned
This project forced me to get comfortable with using tools. I used the GNU Debugger Thread Sanitizer, and Valgrind extensively to diagnose memory leaks and discover race conditions. I had a thankfully brief encounter with a deadlock error when I was first implementing my work queue. Thread Sanitizer made it a lot easier and helped me discover avenues for making progress, which enabled me getting this project finished on time. This project made me think critically about how to manage multiple threads working at the same time. It was incredibly difficult thinking about several programs changing a structure at the same time. Balancing changes that affect other changes in my mind was difficult, but incredibly rewarding when I finally got things working. It was incredibly interesting seeing the performance increases that threading gives you. 
There were some issues with my code that I was not able to fix, chief among them was sometimes, in the last quarter of the array I used to contain the records, a very small fraction of records would be out of order. This did not always happen, so because the issue is transitive I suspect it is because of a race condition. If I had more time, I would try to diagnose and fix this issue. My first guess is I need to explicitly protect concurrent access to the record array with locks instead of relying on each thread only operating within the bounds it is given. 
Additionally, this project cemented my ability to manage heap memory in C. I was forced to get comfortable working with pointers to pointers, structs with pointers, and resizable arrays within structs. This project was a great learning experience for me. At times, correcting seg faults was almost fun, and was always very rewarding. I learned to be very careful when throwing NULL initializations around. It’s better to have zeroed out data instead of something that crashes your program. 
Graphing performance was very interesting to me. Given more time, I want to figure out if I can configure the operating system to let the process take more memory, as it seems like past a certain number of records, the operating system kills the process for requesting too much memory. Making this change would allow me to test my program on larger and larger inputs. I also want to see if there are any machine-dependent optimizations I can leverage to further increase performance. Maybe there’s some trick to creating threads on a Ryzen 5 5600, or tricks to handling the large write call. Maybe aligning the write call to some power of two will give me performance benefits. 
